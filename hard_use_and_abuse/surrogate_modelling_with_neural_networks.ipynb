{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of [surrogate modelling](https://en.wikipedia.org/wiki/Surrogate_model) is as follows. There are several control variables that can be changed by a decision maker and the goal is to find optimal vector of control variables' values, but a loss to be minimized is hard to measure and so its values are available only on limited amount of control variables' vectors. The approach of surrogate modelling lies in building a based on available data model that approximates the loss function on the whole set of available inputs. Then a minimum point of loss function's approximation is regarded as minimum point of real loss.\n",
    "\n",
    "Let us provide an example of situation where surrogate modelling is appropriate. Suppose that engineers are going to create new alloy of two metals and these engineers have two control variables: proportion of one metal to the other and intensity of heat treatment. Assume also that a loss is fragility of alloy. Engineers can produce pieces of various alloys that are obtained with several values of control variables and test them empirically, but this procedure is expensive. After some data are gathered, it is possible to build an approximation of fragility function and then test an alloy that corresponds to its minimum point.\n",
    "\n",
    "In this notebook, machine learning approach is used for surrogate modelling of some functions of two variables. Note that usage of tree-based ensembles is not interesting in this context, because they can not make a prediction that is below target's minimum over training sample or above target's maximum over training sample and, in particular, they can not extrapolate trends. Neural networks are a more intriguing choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from tensorflow import set_random_seed as set_random_seed_for_tf\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set()  # Make all graphs prettier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(361)\n",
    "set_random_seed_for_tf(361)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template of Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designs of Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since functions of just two arguments are studied here, there is no curse of dimensionality. Hence, complex designs such as [Latin hypercubes](https://en.wikipedia.org/wiki/Latin_hypercube_sampling), [Sobol sequences](https://en.wikipedia.org/wiki/Sobol_sequence), or [Halton sequences](https://en.wikipedia.org/wiki/Halton_sequence) are not necessary. Only two designs are used:\n",
    "* Random sampling from uniform distribution on a constrained segment of domain;\n",
    "* Grid of particular size and equal steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_from_uniform_distribution(n_samples, x_borders, y_borders):\n",
    "    \"\"\"\n",
    "    Draws `n_samples` from a uniform\n",
    "    distribution on a rectangle that is\n",
    "    a Cartesian product of intervals with\n",
    "    ends represented by `x_borders` and\n",
    "    `y_borders` respectively.\n",
    "    \n",
    "    @type n_samples: int\n",
    "    @type x_borders: tuple(float)\n",
    "    @type y_borders: tuple(float)\n",
    "    @rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    xs = np.random.uniform(x_borders[0], x_borders[1], n_samples).reshape((-1, 1))\n",
    "    ys = np.random.uniform(y_borders[0], y_borders[1], n_samples).reshape((-1, 1))\n",
    "    return np.hstack((xs, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_grid(step, left_bottom_corner, x_n_steps, y_n_steps):\n",
    "    \"\"\"\n",
    "    Returns array of points from a rectangular\n",
    "    grid with `step` as vertical or horizontal\n",
    "    distance between adjacent nodes. Size of\n",
    "    grid is determined by `x_n_steps` and\n",
    "    `y_n_steps`, while its location is determined\n",
    "    via `left_bottom_corner`.\n",
    "    \n",
    "    @type step: float\n",
    "    @type left_bottom_corner: tuple(float)\n",
    "    @type x_n_steps: int\n",
    "    @type y_n_steps: int\n",
    "    @rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    xs = [left_bottom_corner[0] + i * step for i in range(x_n_steps)]\n",
    "    ys = [left_bottom_corner[1] + i * step for i in range(y_n_steps)]\n",
    "    return np.transpose([np.tile(xs, len(ys)), np.repeat(ys, len(xs))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, brute force over points from above designs is used for surrogate loss optimization. More complex techniques (e.g. gradient-based optimization or [Nelder-Mead method](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method)) are not used, because brute force is computationally feasible due to low dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RunnerOfSurrogateModelling(object):\n",
    "    \"\"\"\n",
    "    Structurizes utilities that are useful\n",
    "    for the following experiment. Given a\n",
    "    function of two real-valued arguments\n",
    "    `func` (actually, `func` must have also\n",
    "    an optional argument `noise_stddev`),\n",
    "    learning sample for this function recovery\n",
    "    is drawn from an experiment that is\n",
    "    designed in accordance with `measurement_doe`.\n",
    "    Then a neural network returned by\n",
    "    `build_model` is trained to approximate `func`\n",
    "    and its optimum is found in accordance\n",
    "    with `optimization_doe`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, noise_stddev, build_model,\n",
    "                 measurement_doe, optimization_doe):\n",
    "        \"\"\"\n",
    "        @type func: function\n",
    "        @type noise_stddev: float\n",
    "        @type build_model: function\n",
    "        @type measurement_doe: str\n",
    "        @type optimization_doe: str\n",
    "        \"\"\"\n",
    "        self.func = func\n",
    "        self.noise_stddev = noise_stddev\n",
    "        self.build_model = build_model\n",
    "        \n",
    "        if measurement_doe == 'random':\n",
    "            self.draw_measurements = draw_from_uniform_distribution\n",
    "        elif measurement_doe == 'grid':\n",
    "            self.draw_measurements = create_grid\n",
    "        else:\n",
    "            raise ValueError(\"Unknown `measurement_doe`: {}\".format(measurement_doe))\n",
    "            \n",
    "        if optimization_doe == 'random':\n",
    "            self.draw_candidates = draw_from_uniform_distribution\n",
    "        elif optimization_doe == 'grid':\n",
    "            self.draw_candidates = create_grid\n",
    "        else:\n",
    "            raise ValueError(\"Unknown `optimization_doe`: {}\".format(optimization_doe))\n",
    "        \n",
    "        self.train_data = None\n",
    "        self.model = None\n",
    "        self.goodness_of_fit = None\n",
    "        self.minimum_point = None\n",
    "        self.actual_value_at_minimum_point = None\n",
    "        self.benchmark = None\n",
    "        \n",
    "    def _draw_train_data(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Draws data according to design of\n",
    "        experiment specified by `kwargs` and\n",
    "        `self.draw_measurements`.\n",
    "        \n",
    "        @rtype: NoneType\n",
    "        \"\"\"\n",
    "        train_inputs = self.draw_measurements(**kwargs)\n",
    "        self.train_data = pd.DataFrame(train_inputs, columns=['x', 'y'])\n",
    "        self.train_data['target'] = self.train_data.apply(\n",
    "            lambda row: self.func(row['x'], row['y'], self.noise_stddev), axis=1)\n",
    "        \n",
    "    def _train_surrogate_model(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Fits `self.model` to `self.train_data`\n",
    "        with specified hyperparameters.\n",
    "        \n",
    "        @rtype: NoneType\n",
    "        \"\"\"\n",
    "        # In Keras, there is no easy way to reset weights without recompiling a model.\n",
    "        self.model = self.build_model()\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(self.train_data[['x', 'y']].as_matrix(),\n",
    "                             self.train_data['target'].as_matrix(),\n",
    "                             random_state=361)\n",
    "        hst = self.model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                             callbacks=[keras.callbacks.History()], verbose=0, **kwargs)\n",
    "        self.goodness_of_fit = {'train_mse': hst.history['loss'][-1],\n",
    "                                'val_mse': hst.history['val_loss'][-1]}\n",
    "        \n",
    "    def _compare_target_vs_surrogate(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Plots two graphs near each other\n",
    "        and computes R^2 coefficient of\n",
    "        determination.\n",
    "        \n",
    "        @rtype: NoneType\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(create_grid(**kwargs), columns=['x', 'y'])\n",
    "        df['target'] = df.apply(\n",
    "            lambda row: self.func(row['x'], row['y'], noise_stddev=0),\n",
    "            axis=1)\n",
    "        df['surrogate'] = df.apply(\n",
    "            lambda row: self.model.predict(row[['x', 'y']].as_matrix().reshape((1, 2)))[0, 0],\n",
    "            axis=1)\n",
    "        self.goodness_of_fit['r2'] = r2_score(df['target'], df['surrogate'])\n",
    "        tqdm.write(\"Evaluation: overall R^2 is {:1.3f}\".format(self.goodness_of_fit['r2']))\n",
    "        \n",
    "        fig = plt.figure(figsize=(15, 7))\n",
    "        \n",
    "        ax_one = fig.add_subplot(121)\n",
    "        ax_one.scatter(df['x'], df['y'], c=df['target'], cmap='coolwarm')\n",
    "        ax_one.set_title('Actual function')\n",
    "        ax_one.set_xlabel('x')\n",
    "        ax_one.set_ylabel('y')\n",
    "        ax_one.set_aspect('equal')\n",
    "        \n",
    "        # TODO: unify colormap scale.\n",
    "        ax_two = fig.add_subplot(122)\n",
    "        ax_two.scatter(df['x'], df['y'], c=df['surrogate'], cmap='coolwarm')\n",
    "        ax_two.set_title('Surrogate function')\n",
    "        ax_two.set_xlabel('x')\n",
    "        ax_two.set_ylabel('y')\n",
    "        ax_two.set_aspect('equal')\n",
    "        \n",
    "    def _compute_benchmark(self):\n",
    "        \"\"\"\n",
    "        Computes benchmark that is a score at\n",
    "        a minimum point of (in general, noisy)\n",
    "        measured target. Ties in values of the\n",
    "        measured target are broken at random.\n",
    "        \n",
    "        @rtype: NoneType\n",
    "        \"\"\"\n",
    "        cond = self.train_data['target'] == self.train_data['target'].min()\n",
    "        potential_inputs = self.train_data.loc[cond, ['x', 'y']]\n",
    "        inputs = potential_inputs.sample(random_state=361).iloc[0, :]\n",
    "        self.benchmark = self.func(inputs['x'], inputs['y'], noise_stddev=0)\n",
    "    \n",
    "    def _find_minimum_of_surrogate_target(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Finds a point with the lowest prediction\n",
    "        of `self.model` amongst points that are\n",
    "        obtained in accordance with\n",
    "        `self.optimization_doe`.\n",
    "        Ties are broken by random choice.\n",
    "        \n",
    "        @rtype: NoneType\n",
    "        \"\"\"\n",
    "        candidates = self.draw_candidates(**kwargs)\n",
    "        candidates = pd.DataFrame(candidates, columns=['x', 'y'])\n",
    "        candidates['surrogate_target'] = candidates.apply(\n",
    "            lambda row: self.model.predict(row.as_matrix().reshape((1, 2)))[0, 0],\n",
    "            axis=1)\n",
    "        \n",
    "        cond = candidates['surrogate_target'] == candidates['surrogate_target'].min()\n",
    "        minimum_points = candidates.loc[cond, ['x', 'y']]\n",
    "        self.minimum_point = minimum_points.sample(random_state=361).iloc[0, :]\n",
    "        self.minimum_point = self.minimum_point.as_matrix().reshape((1, 2))\n",
    "        self.actual_value_at_minimum_point = self.func(\n",
    "            self.minimum_point[0, 0], self.minimum_point[0, 1], noise_stddev=0)\n",
    "        \n",
    "    def run_experiment(self, n_runs, runs_to_be_evaluated, evaluation_settings,\n",
    "                       measurement_settings, train_settings, optimization_settings,\n",
    "                       same_train_sample_for_all_runs=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Launchs `n_runs` experiments defined by\n",
    "        `measurement_settings`, `train_settings`,\n",
    "        and `optimization_settings`.\n",
    "        For experiments that are in\n",
    "        `runs_to_be_evaluated`, learnt surrogate\n",
    "        function is plotted against the actual\n",
    "        function with 'evaluation_settings`\n",
    "        and R^2 score is computed. To avoid\n",
    "        granularity of graphs, one should pass\n",
    "        fine grid and it makes computations\n",
    "        hard, so it is recommended to include\n",
    "        in `runs_to_be_evaluated` only small\n",
    "        proportion of runs.\n",
    "        \n",
    "        @type n_runs: int\n",
    "        @type runs_to_be_evaluated: list(int)\n",
    "        @type evaluation_settings: dict(str -> any)\n",
    "        @type measurement_settings: dict(str -> any)\n",
    "        @type train_settings: dict(str -> any)\n",
    "        @type optimization_settings: dict(str -> any)\n",
    "        @type same_train_sample_for_all_runs: bool\n",
    "        @type verbose: bool\n",
    "        @rtype: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        runs = tqdm(range(n_runs)) if verbose else range(n_runs)\n",
    "        for i in runs:\n",
    "            if self.train_data is None:\n",
    "                self._draw_train_data(**measurement_settings)\n",
    "            \n",
    "            # Even if `same_train_sample_for_all_runs` is `True`,\n",
    "            # benchmark can vary due to ties.\n",
    "            self._compute_benchmark()\n",
    "            \n",
    "            self._train_surrogate_model(**train_settings)\n",
    "            if i in runs_to_be_evaluated:\n",
    "                self._compare_target_vs_surrogate(**evaluation_settings)\n",
    "            self._find_minimum_of_surrogate_target(**optimization_settings)\n",
    "            \n",
    "            benchmark = self.benchmark\n",
    "            score = self.actual_value_at_minimum_point\n",
    "            minimum_point_x = self.minimum_point[0, 0]\n",
    "            minimum_point_y = self.minimum_point[0, 1]\n",
    "            results.append([benchmark, score, minimum_point_x, minimum_point_y])\n",
    "            \n",
    "            if not same_train_sample_for_all_runs:\n",
    "                self.train_data = None\n",
    "        result = pd.DataFrame(results, columns=['benchmark', 'score', 'x', 'y'])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp(hidden_layers_widths, keep_prob, learning_rate):\n",
    "    \"\"\"\n",
    "    Builds Multi-Layer Perceptron (MLP).\n",
    "    \n",
    "    @type hidden_layers_widths: list(int)\n",
    "    @type keep_prob: float\n",
    "    @type learning_rate: float\n",
    "    @rtype: keras.Model\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(\n",
    "        Dense(hidden_layers_widths[0], input_dim=2,\n",
    "              kernel_initializer=RandomNormal(stddev=0.01, seed=361)))\n",
    "    model.add(LeakyReLU(alpha=0.003))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    for i in range(1, len(hidden_layers_widths)):\n",
    "        model.add(\n",
    "            Dense(hidden_layers_widths[i],\n",
    "                  kernel_initializer=RandomNormal(stddev=0.01, seed=361)))\n",
    "        model.add(LeakyReLU(alpha=0.001))\n",
    "        model.add(Dropout(keep_prob))\n",
    "    model.add(\n",
    "        Dense(1, kernel_initializer=RandomNormal(stddev=0.01, seed=361)))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=keras.optimizers.Adam(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Himmelblau's Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of non-convex function with multiple global minima, [Himmelblau's function](https://en.wikipedia.org/wiki/Himmelblau%27s_function) is chosen here.\n",
    "\n",
    "It is: $$f(x, y) = (x^2+y-11)^2 + (x+y^2-7)^2.$$\n",
    "\n",
    "Himmelblau's function is of particular interest, because there is a region where it looks like plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_himmelblaus_function(x, y, noise_stddev=0):\n",
    "    \"\"\"\n",
    "    Computes Himmelblau's function at\n",
    "    point (`x`, `y`) and adds some\n",
    "    Gaussian noise.\n",
    "    \n",
    "    @type x: float\n",
    "    @type y: float\n",
    "    @type noise_stddev: float\n",
    "    @rtype: float\n",
    "    \"\"\"\n",
    "    himmelblaus_value = (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "    noise = np.random.normal(scale=noise_stddev, size=1)[0]\n",
    "    return himmelblaus_value + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target definition.\n",
    "func = noisy_himmelblaus_function\n",
    "noise_stddev = 0.01\n",
    "\n",
    "# Model settings.\n",
    "hidden_layers_widths = [400, 200]\n",
    "keep_prob = 0.95\n",
    "learning_rate = 0.001\n",
    "train_settings = {'epochs': 100,\n",
    "                  'batch_size': 32}\n",
    "\n",
    "# Design of experiment.\n",
    "measurement_doe = 'random'\n",
    "measurement_settings = {'n_samples': 20000,\n",
    "                        'x_borders': (-7, 7),\n",
    "                        'y_borders': (-7, 7)}\n",
    "optimization_doe = 'grid'\n",
    "optimization_settings = {'step': 0.1,\n",
    "                         'left_bottom_corner': (-7, -7),\n",
    "                         'x_n_steps': 141,\n",
    "                         'y_n_steps': 141}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model = partial(build_mlp, *[hidden_layers_widths, keep_prob, learning_rate])\n",
    "runner = RunnerOfSurrogateModelling(\n",
    "    func, noise_stddev, build_model, measurement_doe, optimization_doe)\n",
    "df = runner.run_experiment(n_runs=20, runs_to_be_evaluated=[0],\n",
    "                           evaluation_settings=optimization_settings,\n",
    "                           measurement_settings=measurement_settings,\n",
    "                           train_settings=train_settings,\n",
    "                           optimization_settings=optimization_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate function has low variance within a region that is coloured in deep blue on the right graph. Does it result in wrong placement of minimum points? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['benchmark', 'score']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(-7.5, 7.5)\n",
    "ax.set_ylim(-7.5, 7.5)\n",
    "ax.set_title('Surrogate minimum points and actual minimum points')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "sns.kdeplot(df['x'], df['y'], cmap='Reds', shade=True, shade_lowest=False, ax=ax)\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "himmelblaus_roots = pd.DataFrame([[3, 2],\n",
    "                                  [-2.805118, 3.131312],\n",
    "                                  [-3.77931, -3.283186],\n",
    "                                  [3.584428, -1.848126]],\n",
    "                                  columns=['x', 'y'])\n",
    "_ = ax.scatter(himmelblaus_roots['x'], himmelblaus_roots['y'], marker='x', s=50, c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surrogate minimum points (blue points) are far enough from the nearest to them actual minimum point (represented as a black cross)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenbrock Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try another non-convex function. Namely, it is [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function).\n",
    "\n",
    "Its definition is: $$f(x, y) = (a - x)^2 + b(y - x^2)^2.$$\n",
    "\n",
    "Rosenbrock function is of particular interest, because it has a valley where its values are close to its global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_rosenbrock_function(x, y, noise_stddev=0, a=1, b=100):\n",
    "    \"\"\"\n",
    "    Computes Rosenbrock function\n",
    "    with parameters `a` and `b`at\n",
    "    point (`x`, `y`) and adds some\n",
    "    Gaussian noise.\n",
    "    \n",
    "    @type x: float\n",
    "    @type y: float\n",
    "    @type noise_stddev: float\n",
    "    @type a: float\n",
    "    @type b: float\n",
    "    @rtype: float\n",
    "    \"\"\"\n",
    "    rosenbrock_value = (a - x)**2 + b * (y - x**2)**2\n",
    "    noise = np.random.normal(scale=noise_stddev, size=1)[0]\n",
    "    return rosenbrock_value + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target definition.\n",
    "func = noisy_rosenbrock_function\n",
    "noise_stddev = 0.01\n",
    "\n",
    "# Model settings.\n",
    "hidden_layers_widths = [400, 200]\n",
    "keep_prob = 0.95\n",
    "learning_rate = 0.001\n",
    "train_settings = {'epochs': 100,\n",
    "                  'batch_size': 32}\n",
    "\n",
    "# Design of experiment.\n",
    "measurement_doe = 'random'\n",
    "measurement_settings = {'n_samples': 20000,\n",
    "                       'x_borders': (-2, 2),\n",
    "                       'y_borders': (-1, 3)}\n",
    "optimization_doe = 'grid'\n",
    "optimization_settings = {'step': 0.05,\n",
    "                         'left_bottom_corner': (-2, -1),\n",
    "                         'x_n_steps': 81,\n",
    "                         'y_n_steps': 81}\n",
    "\n",
    "# Making plots less granular.\n",
    "evaluation_settings = {'step': 0.025,\n",
    "                       'left_bottom_corner': (-2, -1),\n",
    "                       'x_n_steps': 161,\n",
    "                       'y_n_steps': 161}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model = partial(build_mlp, *[hidden_layers_widths, keep_prob, learning_rate])\n",
    "runner = RunnerOfSurrogateModelling(\n",
    "    func, noise_stddev, build_model, measurement_doe, optimization_doe)\n",
    "df = runner.run_experiment(n_runs=20, runs_to_be_evaluated=[0],\n",
    "                           evaluation_settings=evaluation_settings,\n",
    "                           measurement_settings=measurement_settings,\n",
    "                           train_settings=train_settings,\n",
    "                           optimization_settings=optimization_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['benchmark', 'score']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(-2.5, 2.5)\n",
    "ax.set_ylim(-1.5, 3.5)\n",
    "ax.set_title('Surrogate minimum points and actual minimum point')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "sns.kdeplot(df['x'], df['y'], cmap='Reds', shade=True, shade_lowest=False, ax=ax)\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "rosenbrock_roots = pd.DataFrame([[1, 1]],\n",
    "                                 columns=['x', 'y'])\n",
    "_ = ax.scatter(rosenbrock_roots['x'], rosenbrock_roots['y'], marker='x', s=50, c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like surrogate minimum points are less stable than in case of Himmelblau's function's modelling, does not it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might think that surrogate modelling does not give any advantages over naive approach at least in 2D case. Although surrogate modelling is an approach suitable for high-dimensional data where grids' sizes grow too fast to allow having fine enough steps, there is an obvious improvement of surrogate modelling's scores. Himmelblau's function is a polynomial and Rosenbrock function is a polynomial as well. Thus, linear regression with squared terms included can model these functions better than the neural networks that are trained above. If you are wondering why neural networks are chosen here as a tool, please note that this notebook is placed in a directory named `hard_use_and_abuse`. Besides that, probably, functions that are not polynomials will be studied here in the future.\n",
    "\n",
    "The last, but not the least, all above experiments are run with quite low standard deviation of noise. An interested reader can increase this parameter and look how performances of both benchmarks and surrogate models degrade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presentations_env",
   "language": "python",
   "name": "presentations_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
