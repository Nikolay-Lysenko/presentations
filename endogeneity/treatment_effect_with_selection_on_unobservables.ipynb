{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data-driven approaches are now used in many fields from business to science. Since data storage and computational power has become cheap, machine learning has gained popularity. However, the majority of tools that can extract dependencies from data, are designed for prediction problem. In this notebook a problem of decision support simulation is considered and it is shown that even good predictive models can lead to wrong conclusions. This occurs under some conditions summarized by an umbrella term called endogeneity. In particular, accuracy of predictions does not guarantee causal relationships detection.\n",
    "\n",
    "Suppose that situation is as follows. There is a freshly-hired manager that can assign treatment to items in order to increase target metric. Treatment is binary, i.e. for each item it is assigned or it is absent. Because treatment costs something, its assignment should be optimized - only some items should be treated. A historical dataset of items performance is given, but the manager does not know that previously treatment was assigned predominantely based on values of just one parameter. Moreover, this parameter is not included in the dataset. To make the situation more weird, an extra assumption can be introduced - now it is impossible to measure values of the omitted parameter not only for old items, but also for a new ones too. By the way, manager wants to create a system that predicts an item's target metric in case of treatment and in case of absence of treatment. If this system is deployed, the manager can compare these two cases and decide whether effect of treatment worths its costs.\n",
    "\n",
    "If machine learning approach results in good prediction scores, chances are that the manager does not suspect that important variable is omitted (at least until some expenses are generated by wrong decisions). Hence, domain knowledge and data understanding are still required for modelling based on data. This is of particular importance when datasets contain values that are produced by someone's decisions, because there is no guarantee that future decisions will not change dramatically. On the flip side, if all factors that affect decisions are included into a dataset, i.e. there is selection on observables for treatment assignment, a powerful enough model is able to estimate treatment effect correctly.\n",
    "\n",
    "Probably, sections of the notebook that illustrate ways to overcome lack of important unobservable variables, will be released after some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read more about causality in data analysis, it is possible to look at these papers:\n",
    "\n",
    "1. *Angrist J, Pischke J-S. Mostly Harmless Econometrics. Princeton University Press, 2009.*\n",
    "\n",
    "2. *Varian H. Big Data: New Tricks for Econometrics. Journal of Economic Perspectives, 28(2): 3â€“28, 2013*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does not use any packages beyond a list of those that are quite popular in scientific computing. Use `conda` or `pip` to install any of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Startup settings can not suppress a warning from XGBRegressor and so this is needed.\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=361)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate an unobservable parameter and an indicator of treatment such that they are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unobservable = np.hstack((np.ones(1000), np.zeros(1000)))\n",
    "treatment = np.hstack((np.ones(900), np.zeros(1000), np.ones(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0.8],\n",
       "       [ 0.8,  1. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(unobservable, treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create historical dataset that is used for learning predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synthesize_dataset(unobservable, treatment,\n",
    "                       first=None, second=None,\n",
    "                       true_weights=np.array([10, 1, 2, 3, 1]).T):\n",
    "    \"\"\"\n",
    "    A helper function for repetitive\n",
    "    pieces of code.\n",
    "    \n",
    "    Creates a dataset, where target depends on\n",
    "    `unobservable`, but `unobservable` is not\n",
    "    included as a feature.\n",
    "\n",
    "    @type unobservable: numpy.ndarray\n",
    "    @type treatment: numpy.ndarray\n",
    "    @type first: numpy.ndarray\n",
    "    @type second: numpy.ndarray\n",
    "    @type true_weights: numpy.ndarray\n",
    "    @return: numpy.ndarray, numpy.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        assert unobservable.shape == treatment.shape\n",
    "    except AssertionError:\n",
    "        raise ValueError(\"Arrays are not aligned.\")\n",
    "    try:\n",
    "        len(true_weights) == 5\n",
    "    except AssertionError:\n",
    "        raise ValueError(\"Not supported length of weights.\")\n",
    "\n",
    "    if first is None:\n",
    "        first = np.random.normal(size=unobservable.shape[0])\n",
    "    if second is None:\n",
    "        second = np.random.normal(size=unobservable.shape[0])\n",
    "    features = np.vstack((unobservable, treatment,\n",
    "                          first, second, first * second)).T\n",
    "    target = np.dot(features, true_weights)\n",
    "    return features[:, 1:], target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_X, learning_y = synthesize_dataset(unobservable, treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create two datasets for simulation where the only difference between them is that in the first one treatment is absent and in the second one treatment is assigned to all items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unobservable = np.hstack((np.ones(250), np.zeros(250)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_treatment = np.zeros(500)\n",
    "full_treatment = np.ones(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_treatment_X, no_treatment_y = synthesize_dataset(unobservable, no_treatment)\n",
    "full_treatment_X, full_treatment_y = synthesize_dataset(unobservable, full_treatment,\n",
    "                                                        no_treatment_X[:, 1],\n",
    "                                                        no_treatment_X[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data that are used for simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.13413915, -1.23892813,  1.4051169 ],\n",
       "       [ 0.        , -0.45949823, -0.13317826,  0.06119518],\n",
       "       [ 0.        , -0.21266401, -0.20021329,  0.04257816],\n",
       "       [ 0.        ,  1.05046456, -0.69384769, -0.72886241],\n",
       "       [ 0.        , -0.48485098,  0.32247408, -0.15635187]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_treatment_X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -1.13413915, -1.23892813,  1.4051169 ],\n",
       "       [ 1.        , -0.45949823, -0.13317826,  0.06119518],\n",
       "       [ 1.        , -0.21266401, -0.20021329,  0.04257816],\n",
       "       [ 1.        ,  1.05046456, -0.69384769, -0.72886241],\n",
       "       [ 1.        , -0.48485098,  0.32247408, -0.15635187]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_treatment_X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.42005421,  8.74266393,  9.01661028,  9.29052366,  9.84136841])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_treatment_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.42005421,   9.74266393,  10.01661028,  10.29052366,  10.84136841])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_treatment_y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfect Model and Poor Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Prediction-vs-fact scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
